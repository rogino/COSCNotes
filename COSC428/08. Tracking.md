# 08. Tracking

## Kalman Filter

Combine noisy measurements with predictions of how the state changes to get better estimate of real state.

Tracking: inference over time.

Can simplify the problem by assuming linear dynamics and Gaussian noise. An unscented Kalman filter can deal with non-linear state transitions, but still assumes Gaussian noise.

Task: at each time point (and in real-time), re-compute the estimate of position.

Recursive estimation: decompose this into:

- The part that depends on the new observation
- The part that can be computed from previous history

Minimal example - running average:

$$
A_t = \alpha a_{t - 1} + (1 - \alpha) y_t
$$

Where $\alpha$ is the weight given to the previous estimate and $y_i$ is the $i$th measurement.

This would be sensitive to noise/occlusion

Tracking:

Generalized model:

- Assume there are moving objects with underlying state $X$ (e.g. position + velocity)
- Assume there are measurements $Y$, some of which are functions of the state
- There is a clock: at each tick, the state changes and we get a new observation

[Issues](http://engr.case.edu/merat_francis/eecs490f06/References/Forsyth/Forsyth_Ch17_Tracking.pdf):

- Data association: the measurements taken at tick $i$ tell us something about the object's state
- Prediction: the measurements $y_0, \cdots, y_{i - 1}$ tells us something about the object's state at tick $i$
  - $P(X_i | Y_0 = y_0, \dots, Y_{i - 1} = y_{i - 1}$
  - Where $Y_i$ is a random variable representing the probability distribution for the $i$th measurement and $y_i$ is the observed measurement
- Correction:
  - Once $y_i$ is obtained, compute $P(X_i | Y_0 = y_0, \dots, Y_i = y_i$

Simplifying assumptions:

- Only the immediate past matters; that is, only the previous state
  - $P(X_i | X_{i - 1})$
- Measurements depend only on the current state
  - Previous measurements do not affect the current measurement

### 1D Kalman Filter

Assumes new state can be obtained by multiplying the old state by a constant $d_i$ and adding noise:

$$
x_i \sim N(d_i x_{i - 1}, \theta_{d_i}^2)
$$

In other words:

$$
\bar{X}^{-}_i = d_i \cdot \bar{X}^{+}_{i - 1} \\
(\theta^{-}_i)^2 = \theta_{d_i}^2 + (d_i \theta^{+}_{i - 1})^2
$$

TODO what is $\theta_{d_i}$? Why not just the second term?

TODO what is $\theta^{+}$ and $\theta^{-}$

Once a measurement arrives, this can be corrected:

$$
x_i^+ = \frac{\bar{x}_i^{-} \theta^2_{m_i} + m_i y_i (\theta^{-}_i)^2}
             {\theta^2_{m_i} + m^2_i (\theta^{-}_i)^2} \\

\theta_i^{+} = \sqrt{\frac{\theta^2_{m_i}(\theta^{-}_i)^2}
                          {\theta^2_{m_i} + m^2_i(\theta_i^{-})^2}}
$$

Note: $\theta$ does not depend on $y$.

Smoothing: if not running the filte in real time, can run the algorithm forwards and backwards and find the mean between the two predictions.

#### [Kalman in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)

g-h filter:

```python
def g_h_filter(data, x0, dx, g, h, dt=1.):
  """
  Performs g-h filter on 1 state variable with a fixed g and h.

  'data' contains the data to be filtered.
  'x0' is the initial value for our state variable
  'dx' is the initial change rate for our state variable (assumes linear rate of change)
  'g' is the g-h's g scale factor. g * 100% of the estimate comes from the measurement. Should be high for less noisy measurements
  'h' is the g-h's h scale factor - larger h means responds quicker to change, but more vulnerable to noise/outliers
  """
  x_estimate = x0
  results = []
  for x_measurement in data:
    # prediction step
    x_prediction = x_estimate + (dx*dt)
    dx = dx

    # update step
    residual = z - x_prediction # delta between measurement and prediction

    # update rate of change using residual.
    # h determines how quickly the rate of change changes
    dx = dx + h * (residual) / dt

    # Update estimate be weighted average of prediction and measurement
    # g determines weight given to the measurement
    x_estimate = x_prediction + g * residual
```

Example: system where position is being measured and the object has constant velocity

The distance and velocity can be represented as Gaussian distributions:

$$
\bar{d} = \mu_d + \mu_v \\
\bar{\theta} = \theta_d^2 + \theta_v^2
$$

Sum of two Gaussians:

$$
\begin{aligned}
\mu &= \mu_1 + \mu_2 \\
\sigma^2 &= \sigma_1^2 + \sigma_2^2
\end{aligned}
$$

Hence, the *prediction* can be represented as the sum of the distributions of the previous position and predicted velocity.

Product of two Gaussians:

$$
\begin{aligned}
\mu &= \frac{\sigma_1^2 \mu_2 + \sigma_2^2 \mu_1}{\sigma_1^2 + \sigma_2^2} \\
\sigma^2 &= \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{aligned}
$$

The *update* step returns the estimated position as the product of the distributions of the new measurement and current estimated position.

## Particle Filter

The particle filter allows multiple positions to be predicted, and works with multi-modal and non-Gaussian distributions.

Three probability distributions:

- Prior density: previous state $p(x_{t - 1})$
- Process density: kinematic model - prediction of next state $p(x_t | x_{t - 1})$
- Observation density: previous observation $p(z_{t - 1} | x_{t - 1})$

The particle filter processes this into a single probability distribution, the **state density** $(x_t | Z_t)$.

Comparisons:

- Kalman filter: linear transitions and Gaussian distributions. Breaks down if there is too much occlusion/clutter
- Unscented Kalman filter: non-linear systems, but still assumes Gaussian distribution
- Particle filter: predicts multiple states/positions with non-Gaussian distribution. Much slower


### [Kalman in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)

Algorithm:

- Generate a bunch of particles randomly
  - Each has a weight proportional to the probability that it represents the state of the real system
  - The use of Monte-Carlo simulation means that particles can be generated which follow any probability distribution, not just the Gaussian
- Predict the next state of the particles based on your predictions of how a real system would behave (e.g. when you send a command to change the state)
  - The changes must be noisy to match the real system
- Update the weighting of the particles based on a new measurement or measurements (e.g. multiple objects being tracked)
  - e.g. for each measurement, find distance between each particle and the measurement, and update the weight accordingly (e.g. for position, add the difference/residual multiplied by some factor to account for the measurements being noisy). Then, normalize the weights so they sum to one
- Resample, discarding particles that are now classed as highly improbable, and generate more particles by duplicating some of the more likely ones
  - The noise added during the predict stage means the duplicate and original will separate
  - If only one particle is being tracked, can use the weighted sum to get an estimate of its real position

