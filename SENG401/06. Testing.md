# 06. Testing

Testing strategies != testing

## Debate: developers should not test their own code/program

Developers should develop, testers should test

Negative: developers should develop and test

Positive:

- Separation of concerns:
  - One team makes, one team breaks
  - Specialization
- Developers are not end users; testers can have a better understanding of the domain and of how users will use the software
- Know what your own code does: may only write tests you know will pass
- Developer may misinterpret requirements and write tests accordingly; a tester will have their own understanding of the requirements
- Gives developer more confidence in their code; experienced tester there to catch bugs
- Will write code that is easier to test as you know that someone else will be looking through it

Negative:

- Latency involved in the back-and-forth between developers and testing
- Counterpoint; writing code that is easier to test if there is a dedicated tester
  - Should be writing easy-to-test code anyway
- Cycle of lots of testing and lots of development; if developers are also testing can switch develop-test workload; can't really do that when there are dedicated testers
- Understanding existing tests helps when writing newer features
- Can't really use TDD if dedicated testers are involved; TDD is iterative, which is hard to do when there are separate teams

Counterpoints against positive:

- Developers should have good understanding of users and problem domain anyway
- Code review process should catch requirements being interpreted
- Having dedicated testers may lead to complacency in code quality and review process

Counterpoints against negative:

- Some industries have strict regulations, require dedicated testers
- Domain knowledge: some is expected, but unrealistic to expect deep domain knowledge from every tester
- Lower bus factor: developer + tester both need to understand the domain

## Quality

- Who creates quality? The developers or the testers?
- Who is responsible for (maintaining) quality?
- When is quality created?

**Quality is created by the developer** - so what is testing for?

Testing isn't about unit testing or integration testing. It is the mindset; a systematic process of:

- Poking and prodding at at system to see how it behaves
- Understanding the limits of a system
- Determining if it behaves as expected
- Determining if it does what is is meant to do; it is **fit-for-purpose**

Testing is about how a user experiences the system and how it compares to our expectations.

In what contexts is testing not required?

- When making a one-off thing (a prototype)
- When it doesn't matter if it works right
  - Zero impact on people's lives or livelihoods
- Small programs

## Hypothesis Testing

The broad steps:

- Conjecture
  - Some sort of expectation informed by your model of the system/world
- Hypothesis (and null hypothesis)
  - A testable conjecture
- Conducting systematic testing of the hypothesis, possibly in multiple ways
- Supporting/rejecting the null hypothesis

Example:

- Model + Conjecture:
  - Logging in is a difficult feature to create securely
  - I have a feeling there is a flaw in the login logic
- Hypothesis:
  - Insecure logins are possible
- Testing:
  - Use 'back button' after logging out
  - Refresh the page
  - Checking if passwords are plain text
  - Sending information as a GET request
  - Logging in as `admin` and `password`
  - Attempting an SQL injection attack
  - Attempting a login with no password
  - etc.

### Verifiability vs Falsifiability

What will it take for us to be able to claim that there are no bugs in the system?

You must test every conceivable avenue and every single branch; *verify* the system. This is almost impossible, although formal proofs are possible in limited domains.

Karl Popper - *The Logic of Scientific Discovery*, 1934.

Verifiability: every single branch can be tested

Falsifiability: at least one example that contradicts the hypothesis can be found

Hence, there is a large asymmetry between the two: when making scientific hypotheses, we find evidence to support or disprove the hypothesis but **we can never prove the hypothesis is true**.

### Testing vs. Automation

Automations help with making the testing process easier; it is not testing itself.

Testing is the *human process* of thinking about how to verify/falsify.

Testing is done in context; humans must intelligently evaluate the results taking this into account.

### Biases

#### Confirmation Bias

The tendency to interpret information in a manner that confirms your own beliefs:

- x is secure
  - In what way? How does it need to be used? What are its limits?
- 100% test coverage means there are no bugs
- Documentation being used to confirm a tester's belief about the SUT (system under testing)
  - Assumes that the SUT's documentation is completely correct
- Positive test bias
  - Testing positive outcomes is *verifying*; instead you should be attempting to *falsify* by choosing tests and data that may lead to negative outcomes


#### Congruence Bias

Subset of confirmation bias, in which people over-rely on their initial hypothesis and neglect to consider alternatives (which may indirectly test the hypothesis).

In testing, this occurs if the tester has strategies that they use all the time and do not consider alternative approaches.

#### Anchoring Bias

Once a baseline is provided, people unconsciously it as a reference point.

Irrelevant information affects the decision making/testing process.

The tester is already anchored in what the system does, perhaps from docs, user stories, talks with management etc. and not consider alternate branches.

**Functional fixedness**: a tendency to only test in the way the system is meant to be used and not think laterally.

#### Law of the Instrument Bias

Believing and relying on an instrument to a fault.

Reliance on the testing tool/methodology e.g. acceptance/unit/integration testing: we use x therefore y must be true.

The way the language is written can affect it as well. e.g. the constrained syntax of user stories leads to complex information and constraints being compressed and relevant information being lost.

#### Resemblance Bias

The toy duck looks like a duck so it must act like a duck: judging a situation based on a similar previous situation

e.g. if you have experience in a similar framework, you may make assumptions about how the current framework works based on your prior experience. This may lead to 'obvious' things being missed or mistaken.

#### Halo Effect Bias

Brilliant people/organizations never make mistakes. Hence, their work does not need to be tested (or this bug I found is a feature, not a bug).

#### Authoritative Bias

- Appealing to authority
- Testers feeling a power level difference when talking to developers
- Listening to what management wants rather than what should be tested
  - Management should be told about the consequences of any steps that are skipped

### Testing Toolbox

TODO Video
